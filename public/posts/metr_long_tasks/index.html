<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Two red-team objections to METR&#39;s research on long tasks  | TuesdayBornWhale</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="AI benchmarks are one attempt to track and formalise the progress of AI&rsquo;s developing capabilities. As explained succinctly in the introduction to &ldquo;Measuring AI Ability to Complete Long Tasks&rdquo; (Kwa et al.), published by METR this year, commonly used benchmarks suffer from a variety of issues, among them that it&rsquo;s hard to track AI progress across time because benchmarks are not often mutually comparable. METR propose a metric which addresses this problem: the X% (task completion) time horizon, which indicates the maximum length of task for a human that an AI can complete X% of the time. I really like this metric as a first attempt to quantitatively come to grips with exactly how quickly AI is developing.">
    <meta name="generator" content="Hugo 0.150.0">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  

    
    
    
      

    

    
    
    <meta property="og:url" content="http://localhost:1313/posts/metr_long_tasks/">
  <meta property="og:site_name" content="TuesdayBornWhale">
  <meta property="og:title" content="Two red-team objections to METR&#39;s research on long tasks ">
  <meta property="og:description" content="AI benchmarks are one attempt to track and formalise the progress of AI’s developing capabilities. As explained succinctly in the introduction to “Measuring AI Ability to Complete Long Tasks” (Kwa et al.), published by METR this year, commonly used benchmarks suffer from a variety of issues, among them that it’s hard to track AI progress across time because benchmarks are not often mutually comparable. METR propose a metric which addresses this problem: the X% (task completion) time horizon, which indicates the maximum length of task for a human that an AI can complete X% of the time. I really like this metric as a first attempt to quantitatively come to grips with exactly how quickly AI is developing.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-16T20:32:28+02:00">
    <meta property="article:modified_time" content="2025-09-16T20:32:28+02:00">

  <meta itemprop="name" content="Two red-team objections to METR&#39;s research on long tasks ">
  <meta itemprop="description" content="AI benchmarks are one attempt to track and formalise the progress of AI’s developing capabilities. As explained succinctly in the introduction to “Measuring AI Ability to Complete Long Tasks” (Kwa et al.), published by METR this year, commonly used benchmarks suffer from a variety of issues, among them that it’s hard to track AI progress across time because benchmarks are not often mutually comparable. METR propose a metric which addresses this problem: the X% (task completion) time horizon, which indicates the maximum length of task for a human that an AI can complete X% of the time. I really like this metric as a first attempt to quantitatively come to grips with exactly how quickly AI is developing.">
  <meta itemprop="datePublished" content="2025-09-16T20:32:28+02:00">
  <meta itemprop="dateModified" content="2025-09-16T20:32:28+02:00">
  <meta itemprop="wordCount" content="1144">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Two red-team objections to METR&#39;s research on long tasks ">
  <meta name="twitter:description" content="AI benchmarks are one attempt to track and formalise the progress of AI’s developing capabilities. As explained succinctly in the introduction to “Measuring AI Ability to Complete Long Tasks” (Kwa et al.), published by METR this year, commonly used benchmarks suffer from a variety of issues, among them that it’s hard to track AI progress across time because benchmarks are not often mutually comparable. METR propose a metric which addresses this problem: the X% (task completion) time horizon, which indicates the maximum length of task for a human that an AI can complete X% of the time. I really like this metric as a first attempt to quantitatively come to grips with exactly how quickly AI is developing.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-dark-gray">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        TuesdayBornWhale
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Blog page">
              Blog
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About me page">
              About me
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        BLOG POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Two red-team objections to METR&#39;s research on long tasks </h1>
      

      
        
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-09-16T20:32:28+02:00">September 16, 2025</time>
      

      
      
        <span class="f6 mv4 dib tracked"> - 6 minutes read </span>
        <span class="f6 mv4 dib tracked"> - 1144 words </span>
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>AI benchmarks are one attempt to track and formalise the progress of AI&rsquo;s developing capabilities. As explained succinctly in the introduction to <a href="https://arxiv.org/abs/2503.14499">&ldquo;Measuring AI Ability to Complete Long Tasks&rdquo;</a> (Kwa et al.), published by METR this year, commonly used benchmarks suffer from a variety of issues, among them that it&rsquo;s hard to track AI progress across time because benchmarks are not often mutually comparable. METR propose a metric which addresses this problem: the X% (task completion) time horizon, which indicates the maximum length of task for a human that an AI can complete X% of the time. I really like this metric as a first attempt to quantitatively come to grips with exactly how quickly AI is developing.</p>
<p>In the paper, METR make an effort to actually &lsquo;prototype&rsquo; this metric in a practical setting, by measuring the development of the 50% and 80% time horizons in LLMs since GPT-2. They do so by employing human baseliners to perform a collection of coding tasks<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, thus garnering an estimate of the time each task takes. They then evaluate the performance of LLMs since GPT-2 on each task and extract a binary value (did they &lsquo;do&rsquo; the task or not).</p>
<p>I personally don&rsquo;t find the object-level results very compelling, as there are many methodological flaws in the way both human baseliners&rsquo; and AI&rsquo;s performance are measured. Many of these are the result of limitations in resources and funding for a tiny non-profit like METR, and many are recognised and discussed thoughtfully in the paper. Overall, I see it as a good pilot that introduces a good idea for meaningfully tracking AI&rsquo;s progress over time, and lays the groundwork for this metric to be measured more robustly with more resources.</p>
<p>In the interest of red-teaming, I&rsquo;ve assembled two objections I have to methodological choices and takeaways in the paper. I tried to choose items which I consider aren&rsquo;t sufficiently addressed, and which i think aren&rsquo;t easily addressed by just &lsquo;scaling up&rsquo; (e.g. just employing more human baseliners).</p>
<h2 id="1-on-context-and-environment">1. On context and environment</h2>
<p>A subset of <a href="https://arxiv.org/pdf/2503.17354">HCAST</a> tasks were given to both agents and human baseliners in <a href="https://vivaria.metr.org/">vivaria</a>, METR&rsquo;s open-source tool for running evaluations on agents. According to Kwa et al, these tasks require &lsquo;much less context&rsquo; than usual software tasks. Moreover, these were selected among other things to need only text input without, for example, visual inputs needed. Finally, all given HCAST tasks are &lsquo;solvable by text editing via a bash shell&rsquo;.</p>
<p>I point these details out to emphasize how the tasks are selected to be tractable for LLMs, and that human baseliners were made to complete these tasks with equally low context and in limited environments. I think this is relevant because humans have a comparative advantage with respect to LLMs in more complex environments and (probably) use context to their advantage in completing real-world tasks.</p>
<p>Kwa et. al recognise that their task suites do not accurately represent real-world problems, and this motivates a mini-experiment comparing the performance of external human baseliners, LLMs
and repository maintainers on some issues in one of METR&rsquo;s internal repositories (this is discussed in section 6.4). They find that the performance of LLMs in these internal tasks is consistent with the paper&rsquo;s findings on HCAST, SWAA and RE-Bench, but only when measured against the low-context human baseliners, as these are 5-18 times slower than the usual repository maintainers. Kwa et al. remark that this suggests their findings are more significant for measuring the progress of AI&rsquo;s X% time-horizon against <em>low-context</em> humans.  However, they don&rsquo;t take the next step, to conclude that the body of their research is measuring the wrong thing. Since most software development is high-context and involves much messier environments which humans have time to get used to, the relevant question for deciding when coding tasks and jobs will be automated is actually how AI compares to human baseliners with high, and not low, context. That&rsquo;s why I think that HCAST specifically is not a very good task-suite for getting the answers we want, and should be replaced (resources permitting) by a suite of tasks which have added context comparable to that seen in the average software development job. Additionally, baseliners should be familiar with that context.</p>
<p>The reason I thought this was an important point to bring up is that I seem to disagree with the authors on a key crux here. In section 8.1, discussing issues with accurately measuring human times for task completions, they write:</p>
<blockquote>
<p>The human baseliners that determine the length of our tasks have much less context than average employees, potentially increasing measured task length. Our tasks are designed to require minimal context, which somewhat mitigates this problem</p></blockquote>
<p>To summarize my argument, tasks requiring minimal context does not mitigate the problem that human times are being calculated using low-context baseliners, it merely hides it.</p>
<hr>
<p>To be fair to METR, even though they don&rsquo;t seem to insist on the importance of comparing AI to high-context humans in this particular paper, their future research actually does meaningfully work on this. In specific, a more recent <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">publication</a> of theirs aims to measure how access to AI impacts human coders who are familiar with the repositories they&rsquo;re working in.</p>
<h2 id="2-on-generalisability-and-coding-is-all-you-need">2. On generalisability and &lsquo;coding is all you need&rsquo;</h2>
<p>To summarise my point bluntly: is coding all you need or not? why or why not?</p>
<p>The task suites used by Kwa et al. are almost exclusively tasks related to software development and machine learning. While the paper recognises this limitation and have since made <a href="https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/">efforts</a> to examine time-horizon development across different domains, they also suggest in section 7.1 that their findings may generalise at least somewhat to other domains.</p>
<p>In part due to many coding tasks needing less context and simpler environments than some other &rsquo;economically valuable&rsquo; tasks, I&rsquo;m not convinced that improvement in the X% time-horizon for coding tasks is likely to generalise well to other domains.</p>
<p>One obvious point in favour of generalisation is the &lsquo;coding is all you need&rsquo; hypothesis, which postulates that improvements in AI coding capabilities should enable automation of large parts of AI R&amp;D, in turn speeding up the development of AI capabilities in other fields. The authors nod towards this possibility in section 7.2.2, and describe &lsquo;substantial AI R&amp;D automation&rsquo; as likely. However, they also seem willing to explore time-horizon development across other fields, which could be interpreted as hedging against this possibility.</p>
<p>What I&rsquo;d love to find out is how much stock Kwa et al. put into the &lsquo;coding is all you need&rsquo; hypothesis, and what kind of research has been done into how plausible it is.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>These are taken from the task suites HCAST, RE-Bench and SWAA, which are described in the paper. The vast majority are either coding tasks or are otherwise software related (for instance, deciding which file name out of four is most likely to contain a password)&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-dark-gray bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  TuesdayBornWhale 2025 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
